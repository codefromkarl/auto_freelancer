#!/usr/bin/env python3
"""
Batch score and analyze projects with iterative feedback loop.

Process:
1. Score un-scored projects in batches of 10
2. After each batch, send results to Gemini and OpenCode for analysis
3. If both agree score >= 7.0, send to Telegram
4. Continue until all projects are scored
"""
import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

script_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(script_dir.parent / "manual_pipeline"))
sys.path.insert(0, str(script_dir.parent / "python_service"))

import common
from database.models import Project
from database.connection import SessionLocal
from services.project_scorer import ProjectScorer, ProjectScore


def format_project_for_analysis(project: Project, score: ProjectScore) -> str:
    """Format project and score for analysis."""
    # Calculate hourly rate for analysis
    budget_min = float(project.budget_minimum) if project.budget_minimum else 0
    budget_max = float(project.budget_maximum) if project.budget_maximum else 0
    avg_budget = (budget_min + budget_max) / 2 if budget_max > 0 else budget_min
    estimated_hours = score.score_breakdown.estimated_hours
    hourly_rate = avg_budget / estimated_hours if estimated_hours > 0 else 0

    return f"""
Project ID: {project.freelancer_id}
Title: {project.title}
Budget: {project.currency_code} {project.budget_minimum} - {project.budget_maximum}
Type: {project.type_id or 'fixed'}
Owner Info: {project.owner_info[:100] if project.owner_info else 'None'}
Bid Count: {project.bid_stats[:100] if project.bid_stats else 'None'}

AI Score: {score.ai_score} ({score.ai_grade})
- Budget Efficiency: {score.score_breakdown.budget_efficiency_score:.2f}
- Estimated Hours: {estimated_hours}
- Hourly Rate: {project.currency_code or 'USD'} {hourly_rate:.2f}/h
- Competition: {score.score_breakdown.competition_score:.2f}
- Clarity: {score.score_breakdown.clarity_score:.2f}
- Customer: {score.score_breakdown.customer_score:.2f}
- Tech: {score.score_breakdown.tech_score:.2f}
- Risk: {score.score_breakdown.risk_score:.2f}

Reason: {score.ai_reason}
"""


def send_to_gemini(projects_data: List[str]) -> str:
    """Send batch results to Gemini for analysis."""
    prompt = f"""è¯·åˆ†æžè¿™æ‰¹é¡¹ç›®çš„è¯„åˆ†ç»“æžœï¼Œåˆ¤æ–­å½“å‰è¯„åˆ†ç³»ç»Ÿæ˜¯å¦å­˜åœ¨é—®é¢˜ã€‚

é¡¹ç›®è¯„åˆ†ç»“æžœï¼ˆå…± {len(projects_data)} ä¸ªï¼‰:
{''.join(['=' * 60] + ['\n\n' + p for p in projects_data] + ['\n'])}

è¯·è¯„ä¼°ï¼š
1. è¯„åˆ†æ˜¯å¦åˆç†ï¼Ÿå¦‚æžœæœ‰æ˜Žæ˜¾ä¸åˆç†çš„è¯„åˆ†ï¼Œè¯·æŒ‡å‡º
2. é¢„ç®—æ•ˆçŽ‡è¯„åˆ†æ˜¯å¦æ­£ç¡®ï¼Ÿ
3. éœ€æ±‚æ¸…æ™°åº¦è¯„åˆ†æ˜¯å¦æ°å½“ï¼Ÿ
4. å·¥ä½œé‡ä¼°ç®—æ˜¯å¦å‡†ç¡®ï¼Ÿ
5. æœ‰ä»€ä¹ˆæ”¹è¿›å»ºè®®ï¼Ÿ

è¯·ç›´æŽ¥ç»™å‡ºä½ çš„åˆ†æžç»“è®ºå’Œæ”¹è¿›å»ºè®®ã€‚"""
    import os
    from config import settings

    # Check if Gemini is enabled
    if not os.getenv("ZHIPU_API_KEY") and not os.getenv("DEEPSEEK_API_KEY"):
        return "Geminiæœªé…ç½®ï¼Œè·³è¿‡åˆ†æž"

    # Use gask-w command for synchronous execution
    import subprocess
    print("  æ­£åœ¨ç­‰å¾… Gemini åˆ†æž...")
    result = subprocess.run(
        ["gask-w", prompt],
        capture_output=True,
        text=True,
        timeout=300  # 5 minutes timeout for analysis
    )
    if result.returncode == 0:
        return result.stdout
    else:
        return f"åˆ†æžå¤±è´¥: {result.stderr}"


def send_to_opencode(projects_data: List[str]) -> str:
    """Send batch results to OpenCode for analysis."""
    prompt = f"""Please analyze this batch of project scoring results and identify any issues with the current scoring system.

Project Scoring Results ({len(projects_data)} projects):
{''.join(['=' * 60] + ['\n\n' + p for p in projects_data] + ['\n'])}

Please evaluate:
1. Are the scores reasonable? Point out any obviously wrong scores
2. Is the budget efficiency scoring correct?
3. Is the requirement clarity scoring appropriate?
4. Is the workload estimation accurate?
5. Any improvement suggestions?

Please provide your analysis conclusions and improvement recommendations directly."""
    import os
    from config import settings

    # Check if OpenCode is available
    if not os.getenv("OPENCODE_API_KEY"):
        return "OpenCodeæœªé…ç½®ï¼Œè·³è¿‡åˆ†æž"

    # Use oask-w command for synchronous execution
    import subprocess
    print("  æ­£åœ¨ç­‰å¾… OpenCode åˆ†æž...")
    result = subprocess.run(
        ["oask-w", prompt],
        capture_output=True,
        text=True,
        timeout=300  # 5 minutes timeout for analysis
    )
    if result.returncode == 0:
        return result.stdout
    else:
        return f"åˆ†æžå¤±è´¥: {result.stderr}"


def send_to_telegram(message: str) -> bool:
    """Send message to Telegram."""
    import os
    from config import settings

    bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID")

    if not bot_token or not chat_id:
        print("Telegramæœªé…ç½®ï¼Œè·³è¿‡é€šçŸ¥")
        return False

    try:
        import requests
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        response = requests.post(url, json={
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }, timeout=30)

        if response.status_code == 200:
            print(f"âœ“ Telegramæ¶ˆæ¯å·²å‘é€")
            return True
        else:
            print(f"âœ— Telegramå‘é€å¤±è´¥: {response.text}")
            return False
    except Exception as e:
        print(f"âœ— Telegramå‘é€å¼‚å¸¸: {e}")
        return False


def process_batch(db, scorer, batch: List[Project], batch_num: int) -> Dict[str, Any]:
    """Process a batch of projects."""
    print(f"\n{'=' * 60}")
    print(f"å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch)} ä¸ªé¡¹ç›®")
    print(f"{'=' * 60}\n")

    scored_results = []
    for project in batch:
        # Build project dict for scoring
        # Handle None values and parse JSON fields safely
        try:
            bid_stats = json.loads(project.bid_stats) if project.bid_stats else {}
        except json.JSONDecodeError:
            bid_stats = {}

        try:
            owner_info = json.loads(project.owner_info) if project.owner_info else {}
        except json.JSONDecodeError:
            owner_info = {}

        # Use full_description if available, otherwise use preview_description
        # The database stores full_description in description field
        description = project.description or project.preview_description or ""

        project_dict = {
            "id": project.freelancer_id,
            "title": project.title,
            "type": "fixed" if project.type_id == 1 else "hourly",
            "budget": {
                "minimum": float(project.budget_minimum) if project.budget_minimum else 0,
                "maximum": float(project.budget_maximum) if project.budget_maximum else 0,
            },
            "currency": {"code": project.currency_code or "USD"},
            "full_description": description,
            "preview_description": project.preview_description or "",
            "bid_stats": bid_stats,
            "owner_info": owner_info,
        }
        score = scorer.score_project(project_dict)
        scored_results.append((project, score))
        print(f"  é¡¹ç›® {project.freelancer_id}: {score.ai_score} ({score.ai_grade}) - {score.ai_reason[:50]}...")

        # Update database
        from services import project_service
        project_service.update_project_ai_analysis(
            db,
            project.freelancer_id,
            score.ai_score,
            score.ai_reason,
            score.ai_proposal_draft,
            None  # suggested_bid not needed for rule-based scoring
        )

    # Commit batch updates
    db.commit()

    # Prepare data for analysis
    projects_data = [format_project_for_analysis(p, s) for p, s in scored_results]
    high_score_projects = [(p, s) for p, s in scored_results if s.ai_score >= 7.0]

    # Send to analysis
    print(f"\n--- æŽ¨é€ç»™ Gemini åˆ†æž ---")
    gemini_result = send_to_gemini(projects_data)
    print(f"Geminiåˆ†æž: {gemini_result[:200]}..." if len(gemini_result) > 200 else gemini_result)

    print(f"\n--- æŽ¨é€ç»™ OpenCode åˆ†æž ---")
    opencode_result = send_to_opencode(projects_data)
    print(f"OpenCodeåˆ†æž: {opencode_result[:200]}..." if len(opencode_result) > 200 else opencode_result)

    # Check if analysis indicates that current scoring needs improvement
    needs_improvement = False
    improvement_reasons = []

    # Check Gemini analysis for issues (Gemini is the primary analyzer)
    gemini_lower = gemini_result.lower()
    issue_keywords = ["ä¸åˆç†", "é—®é¢˜", "éœ€è¦æ”¹è¿›", "ä¸æ­£ç¡®", "åä½Ž", "åé«˜", "error", "issue", "improve", "unreasonable", "incorrect", "å¤±æ•ˆ", "ç¼ºé™·", "åå·®", "å´©æºƒ", "é€†å‘", "äºæœ¬"]

    gemini_has_issues = any(kw in gemini_lower for kw in issue_keywords)

    # Also check OpenCode if available (secondary analysis)
    opencode_has_issues = False
    if not opencode_result.startswith("OpenCodeæœªé…ç½®"):
        opencode_lower = opencode_result.lower()
        opencode_has_issues = any(kw in opencode_lower for kw in issue_keywords)

    # Stop if Gemini identifies issues (primary analyzer)
    if gemini_has_issues:
        needs_improvement = True
        if opencode_has_issues:
            improvement_reasons = ["Gemini å’Œ OpenCode éƒ½è®¤ä¸ºå½“å‰è¯„åˆ†ç³»ç»Ÿéœ€è¦ä¼˜åŒ–"]
        else:
            improvement_reasons = ["Gemini è®¤ä¸ºå½“å‰è¯„åˆ†ç³»ç»Ÿéœ€è¦ä¼˜åŒ–"]
        print(f"\n{'âš ï¸' * 30}")
        print("âš ï¸ æ£€æµ‹åˆ°è¯„åˆ†ç³»ç»Ÿéœ€è¦ä¼˜åŒ–ï¼")
        print(f"{'âš ï¸' * 30}")
        print(f"\nGeminiåˆ†æžæ‘˜è¦: {gemini_result[:400]}...")
        if opencode_has_issues:
            print(f"OpenCodeåˆ†æžæ‘˜è¦: {opencode_result[:400]}...")

    # Check if both agree on high scores
    # Simplified check: if AI score >= 7.0, send notification
    if high_score_projects:
        project_list = "\n".join([
            f"  - {p.title} (ID: {p.freelancer_id}, è¯„åˆ†: {s.ai_score})"
            for p, s in high_score_projects
        ])
        telegram_msg = f"""ðŸŽ¯ é«˜åˆ†é¡¹ç›®é€šçŸ¥ (è¯„åˆ† >= 7.0)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{project_list}

ðŸ”— æŸ¥çœ‹è¯¦æƒ…"""
        send_to_telegram(telegram_msg)

    return {
        "batch_num": batch_num,
        "count": len(batch),
        "high_score_count": len(high_score_projects),
        "gemini_analysis": gemini_result[:500],
        "opencode_analysis": opencode_result[:500],
    }


def main(argv=None) -> int:
    parser = argparse.ArgumentParser(description="Batch score and analyze projects.")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size (default: 10)")
    parser.add_argument("--lock-file", default=str(common.DEFAULT_LOCK_FILE), help="Lock file path")
    args = parser.parse_args(argv)

    logger = common.setup_logging("batch_score_analyze")

    with common.file_lock(Path(args.lock_file), blocking=False) as acquired:
        if not acquired:
            print("Lock busy. Another workflow may be running.")
            return common.EXIT_LOCK_ERROR

        try:
            common.load_env()
            common.get_settings()
        except Exception as exc:
            print(f"Failed to load settings: {exc}")
            return common.EXIT_VALIDATION_ERROR

        with common.get_db_context() as db:
            # Get total un-scored projects
            total_unscored = (
                db.query(Project)
                .filter(Project.ai_score.is_(None))
                .count()
            )

            if total_unscored == 0:
                print("æ‰€æœ‰é¡¹ç›®å·²è¯„åˆ†å®Œæˆï¼")
                return common.EXIT_SUCCESS

            print(f"æ€»å¾…è¯„åˆ†é¡¹ç›®æ•°: {total_unscored}")
            print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
            print(f"é¢„è®¡éœ€è¦ {(total_unscored + args.batch_size - 1) // args.batch_size} ä¸ªæ‰¹æ¬¡\n")

            # Process in batches
            scorer = ProjectScorer()
            batch_num = 0
            all_results = []
            needs_optimization = False

            while True:
                # Get next batch
                batch = (
                    db.query(Project)
                    .filter(Project.ai_score.is_(None))
                    .order_by(Project.created_at.desc())
                    .limit(args.batch_size)
                    .all()
                )

                if not batch:
                    break

                batch_num += 1
                result = process_batch(db, scorer, batch, batch_num)
                all_results.append(result)

                # Check if this batch indicates needs optimization
                if "needs_improvement" in result and result["needs_improvement"]:
                    needs_optimization = True
                    # Stop and ask for optimization
                    break

                # Pause between batches
                if len(batch) == args.batch_size:
                    print(f"\næ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œç­‰å¾… 3 ç§’åŽç»§ç»­...")
                    import time
                    time.sleep(3)

            # If optimization needed, stop and wait
            if needs_optimization:
                print(f"\n{'âš ï¸' * 40}")
                print("âš ï¸ æ£€æµ‹åˆ°è¯„åˆ†ç³»ç»Ÿéœ€è¦ä¼˜åŒ–ï¼")
                print("âš ï¸ è¯·å…ˆä¼˜åŒ–è¯„åˆ†ç³»ç»ŸåŽå†ç»§ç»­è¯„åˆ†")
                print(f"{'âš ï¸' * 40}")
                return common.EXIT_VALIDATION_ERROR

            # Final summary
            print(f"\n{'=' * 60}")
            print(f"æ‰¹é‡è¯„åˆ†å®Œæˆï¼")
            print(f"{'=' * 60}")
            print(f"æ€»æ‰¹æ¬¡æ•°: {len(all_results)}")
            print(f"æ€»è¯„åˆ†é¡¹ç›®æ•°: {sum(r['count'] for r in all_results)}")
            high_score_count = sum(r['high_score_count'] for r in all_results)
            print(f"é«˜åˆ†é¡¹ç›®æ•° (>=7.0): {high_score_count}")

            return common.EXIT_SUCCESS


if __name__ == "__main__":
    sys.exit(main())
